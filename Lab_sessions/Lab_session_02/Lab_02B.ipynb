{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEN122A Statistical Analysis of Choice Behaviour \n",
    "\n",
    "## `Lab session 02B:`\n",
    "## `The Mixed Logit model`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2025**<br>\n",
    "**Instructor:** Sander van Cranenburgh<br>\n",
    "**TA:**  Gabriel Nova <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Lab sessions aim to:**<br>\n",
    "* Illustrate how models and theory discussed in the classroom work out in practice.\n",
    "* Help you gather hands-on modelling and data analysis skills.\n",
    "\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Python and get support from TA and fellow students.\n",
    "* Not graded and do not have to be submitted.\n",
    "* A good preparation for the graded partial exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Application: Estimating the Value of Travel Time`\n",
    "\n",
    "In this lab session, we will investigate the \"Value of Travel Time\" (VTT) distribution. The VTT of a traveller reflects the amount of money the traveller is **willing to pay** to reduce their travel time. The VTT is used to determine the benefits of new infrastructure projects. As travel time savings are the dominant and most salient benefits of new infrastructure, accurate inference of the distribution of the VTT is crucial for a rigorous underpinning of policy decisions. <br>\n",
    "\n",
    "During this lab, we will apply Mixed Logit choice models. We aim to uncover how tastes for travel time and travel cost are distributed in the population. Most of the analyses in this lab session are carried out in the so-called willingness-to-pay space. Willingness-to-pay space facilitates the inference of the VTT distribution.<br>\n",
    "\n",
    "For this study, we will use Stated Choice (SC) data (`Norway_VTT_2009.csv`) collected in 2009 to compute the Norwegian VTT. In this SC experiment, respondents faced nine choice tasks involving two alternatives and two attributes (travel cost and travel time). The data set consists of 5,832 participants, resulting in a total of 52,488 choice observations. The figure below shows one of the choice tasks (note that for the purposes of illustration we converted the currency unit (Kronor) into euros).\n",
    "\n",
    "![SC](assets/sc_experiment.png)\n",
    "\n",
    "**`Learning objectives lab session 02B`**\n",
    "\n",
    "After completing the following exercises, you will be able to:\n",
    "* Estimate Mixed Logit models that account for panel data\n",
    "* Discuss the impact of the number of draws on the modelling outcomes\n",
    "\n",
    "\n",
    "**`This lab consists of 2 parts and has 2 exercises`**\n",
    "\n",
    "**Part 1**: The Panel Mixed Logit model\n",
    "\n",
    "- Exercise 1: \"Panel MXL model with log-normally distributed VTT\"\n",
    "\n",
    "**Part 2**: Impact of the number of draws on modelling outcomes\n",
    "\n",
    "- Exercise 2: \"Impact of the number of draws\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Import packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will import all the libraries that we will use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biogeme\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.biogeme_logging as blog\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta, Variable, bioDraws, log, MonteCarlo, exp, bioMultSum, exp\n",
    "\n",
    "# General packages\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy.stats import norm, lognorm\n",
    "\n",
    "# Pandas setting to show all columns when displaying a pandas dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the utils folder to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "\n",
    "# Import the bio_estimation_fcns from the utils folder\n",
    "from utils.bio_estimation_fcns import print_results, estimate_panel_mxl, plot_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invoke a so-called `logger` which enables us to see the progress during estimation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger, if it has not been initialized yet\n",
    "try:\n",
    "    logger\n",
    "except NameError:    \n",
    "    logger = blog.get_screen_logger(level=blog.INFO)\n",
    "    print('Logger has been initialised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1. Load and explore the data set` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same data set as in lab session 2A. So,we load the data and process it similarly as in lab session 2A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data set\n",
    "data_path = Path('data/Norway_VTT_2009.csv')\n",
    "df = pd.read_table(data_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Keep only entries purpose == 5 (Long distance trips) & Mode == 1 (Car)\n",
    "df = df.loc[(df['Purpose'] == 5) & (df['Mode'] == 1)]\n",
    "\n",
    "# Keep only the relevant columns\n",
    "df.drop(columns=['Mode','Purpose', 'Gender', 'AgeClass', 'IncClass'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert the monetary unit to euros\n",
    "NOK2euro_exchange_rate = 9\n",
    "df[['CostL','CostR']] = df[['CostL','CostR']] .div(NOK2euro_exchange_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1. The Panel Mixed Logit model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have worked on the assumption that each choice observation is uncorrelated with all other choice observations. However, this data set contains multiple choices per respondent. In the MXL modelling framework, we can also account for correlation in unobserved utility **across observations** of the same individual if we specify it as a panel MXL model. In the panel MXL model, the likelihood of the sequence of choices *t* = 1..*T* of an individual *n* is given by:  \n",
    "\n",
    "$L_n(i_1,...,i_{T})(\\beta_n|\\sigma) = \\int_{\\beta_n}\\Pi_{t=1}^T     P_{n}(i_t|\\beta_n) f(\\beta_n|\\sigma)d\\beta_n$\n",
    "\n",
    "This likelihood does not have a closed-form expression. Therefore, as before, it needs to be approximated using simulation. Let's re-estimate the MXL model assuming a normally distributed VTT distribution while accounting for panel structure. To do this, we first need to convert the data set into a so-called wide data format. In a wide format data set, each row contains all the choices belonging to an individual. Conveniently, Biogeme has a built-in function to do this (but, rather inconveniently, the names of the columns still need to be renamed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.1. Preparing a wide Biogeme database for estimating panel MXL model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we transform our data set into a wide format, and create a new Biogeme database object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Biogeme database object\n",
    "biodata = db.Database('Norway2009VTT', df)\n",
    "\n",
    "# Tell Biogeme which variable is the identifier of the individuals\n",
    "biodata.panel('RespID')\n",
    "\n",
    "# Calculate the number of observations per individual\n",
    "obs_per_ind = biodata.data['RespID'].value_counts().unique()[0]\n",
    "print(f'Number of observations per individual: {obs_per_ind}')\n",
    "\n",
    "# Use biogeme's \"generateFlatPanelDataFrame to create a wide database in which each row corresponds to one individual\n",
    "df_wide = biodata.generate_flat_panel_dataframe(identical_columns=None)\n",
    "\n",
    "# Rename the columns, such that they run from columnname_{0} to columnname_{n} \n",
    "renumbered_columns = {col: f'{col.split(\"_\")[1]}_{int(col.split(\"_\")[0])-1}' if len(col.split(\"_\")) == 2 else col for col in df_wide.columns}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "df_wide.rename(columns=renumbered_columns, inplace=True)\n",
    "\n",
    "# list of df_wide.columns excluding the columns that start with 'Cost'\n",
    "col_in = df_wide.columns[df_wide.columns.str.startswith('Cost') == False].tolist()\n",
    "# Convert the columns that do not start with 'Cost' to integers\n",
    "for col in col_in:\n",
    "    df_wide[col] = df_wide[col].astype(int)\n",
    "\n",
    "# Create Biogeme database object\n",
    "biodata_wide = db.Database('Norway2009VTT_wide', df_wide)\n",
    "\n",
    "# Show the first rows of the wide database\n",
    "print(f'The wide dataset has a shape of {biodata_wide.data.shape}')\n",
    "biodata_wide.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionaries with the Biogeme variables from the wide database\n",
    "CostL   =   {q: Variable(f'CostL_{q}') for q in range(obs_per_ind)}\n",
    "TimeL   =   {q: Variable(f'TimeL_{q}') for q in range(obs_per_ind)}\n",
    "CostR   =   {q: Variable(f'CostR_{q}') for q in range(obs_per_ind)}\n",
    "TimeR   =   {q: Variable(f'TimeR_{q}') for q in range(obs_per_ind)}\n",
    "CHOICE  =   {q: Variable(f'Chosen_{q}') for q in range(obs_per_ind)}\n",
    "\n",
    "# Inspect one of the dictionaries\n",
    "CHOICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.2. Panel MXL model with normally distributed VTT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the model a name\n",
    "model_name = 'Panel MXL WTP space with normally distributed vtt'\n",
    "\n",
    "# Parameters definition enabling the construction of random parameters\n",
    "vtt       = Beta('vtt',       0.4, None, None, 0)\n",
    "B_tc      = Beta('b_tc',     -0.4, None, None, 0)    \n",
    "sigma_vtt = Beta('sigma_vtt',   2, None, None, 0)\n",
    "\n",
    "# Construction of random parameters   \n",
    "vtt_rnd = vtt + sigma_vtt * bioDraws('vtt_rnd', 'NORMAL_HALTON2')\n",
    "\n",
    "# Definition of the utility functions\n",
    "# Note that we use list comprehension to create a list of utility functions for all observations of an individual \n",
    "V_L = [B_tc * (CostL[q] + vtt_rnd * TimeL[q]) for q in range(obs_per_ind)]\n",
    "V_R = [B_tc * (CostR[q] + vtt_rnd * TimeR[q]) for q in range(obs_per_ind)]\n",
    "\n",
    "# Create a dictionary to list the utility functions with the numbering of alternatives\n",
    "# Note that we use list comprehension to create a list of dictionaries\n",
    "V = [{1: V_L[q], 2: V_R[q]} for q in range(obs_per_ind)]\n",
    "           \n",
    "# Create a dictionary to describe the availability conditions of each alternative\n",
    "AV = {1:1, 2:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the model using the estimate_panel_mxl function\n",
    "results_wtp_panel_mxl_normal = estimate_panel_mxl(V,AV,CHOICE,obs_per_ind,biodata_wide,model_name,num_draws = 100)\n",
    "\n",
    "# Print the results\n",
    "print_results(results_wtp_panel_mxl_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the value of travel time\n",
    "beta_hat_wtp_panel_mxl_normal = results_wtp_panel_mxl_normal.get_beta_values()\n",
    "VTT_wtp_panel_mxl_normal = 60 * beta_hat_wtp_panel_mxl_normal['vtt']\n",
    "print(f'Value of travel time Panel MXL model in WTP space:  â‚¬{VTT_wtp_panel_mxl_normal:.2f} per hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the distribution of the VTT distribution\n",
    "# Create the dictionary with the distribution types\n",
    "distr_types = {\n",
    "    'vtt':  {'dist': 'normal'},\n",
    "}\n",
    "# Plot the distributions of the random parameters, by passing it the biogeme results, the dictionary with the distribution types, and the x-axis limits\n",
    "# Note that the unit of the VTT distribution is euros per minute\n",
    "plot_distributions(results_wtp_panel_mxl_normal, distr_types, xmin = -2, xmax = 120/60, xlabel='VTT (euros per minute)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Exercise 1: Panel MXL with log-normally distributed VTT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **you** will estimate a MXL model under the assumption that the VTT is log-normally distributed, while accounting for panel effects.<br>\n",
    "\n",
    "To do so, copy the code from the Panel MXL model in WTP space with normally distributed VTT, and create the log-normally distributed random parameter (as you have done in exercise 2 of lab_session 2A).<br>  \n",
    "Estimate this model and interpret the results.<br>\n",
    "\n",
    "\n",
    "`A` Compare the log-likelihood of the MXL models with the log-normally distributed VTTs, which do and do not account for the panel effect. Which model fits better?<br>\n",
    "\n",
    "`B` Compute the mean of the VTT for the Panel MXL model with the log-normally distributed VTT and compare it with the non-panel model. Has it changed?<br>\n",
    "\n",
    "`C`  i. Print the recovered mean VTTs of the models we have estimated below each other.<br>\n",
    "* MNL model<br>\n",
    "* MXL model with Normal distribution in utility space<br>\n",
    "* MXL model with Normal distribution in wtp space<br>\n",
    "* MXL model with Log-normal in wtp space<br>\n",
    "* Panel MXL with Normal distribution in wtp space<br>\n",
    "* Panel MXL with Log-normal distribution in wtp space<br>                     \n",
    "\n",
    "ii. Compare the VTTs of the models with a normal distribution and a log-normal distribution. Do you see a pattern? <br>\n",
    "\n",
    "iii. What could explain this pattern?<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2. Impact of the number of draws on modelling outcomes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Exercise 2: Impact of the number of draws` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the Mixed Logit models that we have estimated, we have used a low number of draws (`num_draws = 100`). We choose a relatively low number of draws to avoid long estimation times.  <br>\n",
    "\n",
    "Next, we analyse how sensitive the modelling outcomes are towards the number of draws. To do this, we have estimated a Panel Mixed Logit model using different numbers of draws, ranging from 33 to 2,000, and stored the results. <br>\n",
    "\n",
    "The following plots show the results. \n",
    "\n",
    "![Draws](assets/draws_vs_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Questions:`\n",
    "\n",
    "`A` The left-hand side plot shows that the VTT estimate gets more stable with an increasing number of draws. Can you explain why the estimate gets more stable? \n",
    "\n",
    "`B` What number of draws do you deem sufficient for estimating this model? Explain your answer.\n",
    "\n",
    "`C` The right-hand side plot shows a linear relation between the number of draws and the estimation time. Explain why a linear relation was to be expected.\n",
    "\n",
    "`D` Suppose we estimate a model with *K* random parameters. Would the relation between the number of draws and estimation time still be linear? Explain your answer. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q2_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
